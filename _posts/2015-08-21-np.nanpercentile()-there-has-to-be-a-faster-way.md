---
published: false
layout: post
title: np.nanpercentile() - there has to be a faster way!
---

Recently I was trying to calculate the quantiles of the vegetation index of an area over time. For this I have a time-series of satellite raster images of a certain region that cover identical extents. This is represented as a `numpy array` of the `shape(96, 4800, 4800)` - in other words 96 satellite images each with the size of 4800 by 4800 pixels. If I want to calculate the 10th, 25th, 50th, 75th and 90th quantile along the time/z-axis this can be done easily with `np.percentile(a, q=[10,25,50,75,90], axis=0)`. The data I am working with contains no_data areas due to residual cloud cover, rainfall, etc. represented as `np.NaN`. Naturally I was turning to numpys `np.nanpercentile(a, q=[10, 25, 50, 75, 90], axis=0)`. **Unfortunately np.nanpercentile() was ~300x slower on my dataset than np.percentile() so I had to find a way to speed it up.**

## np.nanpercentile() speed

Generate some test data and benchmark numpys function to see what we are working with:

```python
# create array of shape(5,100,100) - image of size 10x10 with 5 layers
test_arr = np.random.randint(0, 10000, 50000).reshape(5,100,100).astype(np.float32)
np.random.shuffle(test_arr)
# place random NaN
rand_NaN = np.random.randint(0, 50000, 500).astype(np.float32)
for r in rand_NaN:
    test_arr[test_arr == r] = np.NaN
```
```python
%timeit np.nanpercentile(test_arr, q=25, axis=0)
100 loops, best of 3: 5.92 ms per loop
```
The reason `np.nanpercentile()` is so slow in my case can be found in the source code. Numpys implementation includes a private function to calculate the percentile along a 1D array while ignoring NaNs. This means that it has to loop through all pixels resulting in 4800 x 4800 native Python loops. Not exactly a recipe for speed.

## The alternative: a pure Python implementation of the quantile calculation

Since we are working with just 3 dimensions we can use a pure Python implementation of the quantile function and should see a significant increase in speed (sounds wrong, I know!). The code for this was posted in a [stackoverflow answer](http://stackoverflow.com/questions/2374640/how-do-i-calculate-percentiles-with-python-numpy).

The general idea is to:
 - find the number of valid observations (non NaN)
 - replace NaN with maximum value of array
 - sort values along axis
 - find position of quantile regarding number of valid observations
 - linear interpolation of if the desired quantile is inbetween two positions (like numpys linear interpolation)
